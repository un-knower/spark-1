package clworks

import org.apache.spark.mllib.feature.{HashingTF, IDF}
import org.apache.spark.mllib.linalg.{SparseVector => SV}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext
import breeze.linalg._
import org.apache.log4j.{Level, Logger}

import scala.io.Source

object TfidfSimilarity {
  def main(args: Array[String]): Unit = {

    Logger.getLogger("org").setLevel(Level.ERROR)


    val conf = new SparkConf().setAppName("df").setMaster("local[2]")
    val sc = new SparkContext(conf)
    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._

    //计算两个文档相似度

    /**
      * 每行作为一个document,zipWithIndex将每一行的行号作为doc的id,生成
      * (17/12/28 17:41:32 INFO SecurityManager: Changing view acls to: ChuangLan,6)
       (17/12/28 17:41:32 INFO SecurityManager: Changing modify acls to: ChuangLan,7)
      */
    val doc=sc.parallelize(Source.fromFile("data/CHANGELOG").getLines().filter(_.trim.length>1).toSeq).zipWithIndex()

    /**
      * 转换为tf向量
      * (0,(1048576,[32,33,45,46,47,49,52,54,55,58,66,68,70,74,76,83,91,93,97,98,99,100,101,102,103,105,106,108,109,110,111,112,114,115,116,117,118],[4.0,1.0,3.0,6.0,11.0,2.0,5.0,2.0,2.0,4.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,4.0,1.0,2.0,3.0,5.0,5.0,5.0,9.0,6.0,9.0,4.0,6.0,4.0,3.0,6.0,9.0,2.0,1.0,1.0]))
      * 1048576，前面数组代表向量值，后面的数组代表出现的次数
      * Math.pow(2, 18).toInt参数意义：默认特征维度是20，可有可无
      */
    val hashingTF=new HashingTF(Math.pow(2, 18).toInt)
    val tf_num_pairs=doc.map{
      case (seq,num)=>
        val tf=hashingTF.transform(seq)
        (num,tf)
    }


    /**
      * 将tf向量转换成tf-idf向量
      *(0,(262144,[32,33,45,46,47,49,52,54,55,58,66,68,70,74,76,83,91,93,97,98,99,100,101,102,103,105,106,108,109,110,111,112,114,115,116,117,118],[0.0,1.2992829841302609,3.0348027350354396,2.7119107424583433,1.0484119778475742,0.6369074622370692,0.0,3.4094961844768505,0.6369074622370692,0.0,1.2992829841302609,1.2992829841302609,0.19062035960864987,0.7884573603642703,0.6369074622370692,0.19062035960864987,1.0116009116784799,1.0116009116784799,0.0,0.6061358035703155,0.19062035960864987,0.9553611933556038,0.0,1.5922686555926728,0.0,0.0,3.636814821421893,0.8577916182389245,1.2738149244741384,0.0,0.0,0.6020120863864538,0.0,0.0,0.0,0.0,0.4519851237430572]))
      * 广播一份tf-idf向量集
      */
    val idf = new IDF().fit(tf_num_pairs.values)
    val num_idf_pairs = tf_num_pairs.mapValues(v => idf.transform(v))
    val b_num_idf_pairs = sc.broadcast(num_idf_pairs.collect())

    //当文本只有一条记录时，Idf值全为0
    //num_idf_pairs.foreach(println(_))


    //计算doc之间相似度

        /**
          * SparseVector同时在breeze.linalg包下和org.apache.spark.mllib.linalg.SparseVector包下，造成歧义
          * 解决方式：为有歧义的类重命名
          *
          import java.util.{ HashMap=>JavaHashMap }   --对应java包的类
          import scala.collection.mutable._    --对应scala包的类
          */

    /**
      *  在文本分析时使用稀疏矩阵
      *  1.num_idf_pairs 为RDD[(Long,Vector类型)]，将Vector类型转换为SparseVector
      */

    //(0,(262144,[32,33,45,46,47,49,52,54,55,58,66,68,70,74,76,83,91,93,97,98,99,100,101,102,103,105,106,108,109,110,111,112,114,115,116,117,118],[0.0,1.2992829841302609,3.0348027350354396,2.7119107424583433,1.0484119778475742,0.6369074622370692,0.0,3.4094961844768505,0.6369074622370692,0.0,1.2992829841302609,1.2992829841302609,0.19062035960864987,0.7884573603642703,0.6369074622370692,0.19062035960864987,1.0116009116784799,1.0116009116784799,0.0,0.6061358035703155,0.19062035960864987,0.9553611933556038,0.0,1.5922686555926728,0.0,0.0,3.636814821421893,0.8577916182389245,1.2738149244741384,0.0,0.0,0.6020120863864538,0.0,0.0,0.0,0.0,0.4519851237430572]))

    //rdd取出索引和向量的方法
    /**
      *  将idf产生的向量转换为稀疏向量，便于使用余弦公式，使用breeze.linalg包中的SparseVector
      * map结果：(0,SparseVector((33,1.2992829841302609), (45,3.0348027350354396), (46,2.7119107424583433), (47,1.0484119778475742), (49,0.6369074622370692), (54,3.4094961844768505), (55,0.6369074622370692), (66,1.2992829841302609), (68,1.2992829841302609), (70,0.19062035960864987), (74,0.7884573603642703), (76,0.6369074622370692), (83,0.19062035960864987), (91,1.0116009116784799), (93,1.0116009116784799), (98,0.6061358035703155), (99,0.19062035960864987), (100,0.9553611933556038), (102,1.5922686555926728), (106,3.636814821421893), (108,0.8577916182389245), (109,1.2738149244741384), (112,0.6020120863864538), (118,0.4519851237430572)))
      * flatmap结果：每个结果都扁平化
0
SparseVector((33,1.2992829841302609), (45,3.0348027350354396), (46,2.7119107424583433), (47,1.0484119778475742), (49,0.6369074622370692), (54,3.4094961844768505), (55,0.6369074622370692), (66,1.2992829841302609), (68,1.2992829841302609), (70,0.19062035960864987), (74,0.7884573603642703), (76,0.6369074622370692), (83,0.19062035960864987), (91,1.0116009116784799), (93,1.0116009116784799), (98,0.6061358035703155), (99,0.19062035960864987), (100,0.9553611933556038), (102,1.5922686555926728), (106,3.636814821421893), (108,0.8577916182389245), (109,1.2738149244741384), (112,0.6020120863864538), (118,0.4519851237430572))
1
SparseVector((33,1.2992829841302609), (45,5.058004558392399), (46,2.7119107424583433), (47,1.2390323374562242), (50,1.2738149244741384), (56,0.6369074622370692), (66,1.2992829841302609), (68,1.2992829841302609), (70,0.19062035960864987), (74,0.7884573603642703), (76,0.6369074622370692), (83,0.19062035960864987), (91,1.0116009116784799), (93,1.0116009116784799), (98,0.6061358035703155), (99,0.2859305394129748), (100,0.9553611933556038), (102,1.2738149244741384), (104,0.3184537311185346), (106,4.849086428562524), (108,1.1437221576518992), (109,1.2738149244741384), (112,0.802682781848605), (118,0.4519851237430572))
      *
      */

    b_num_idf_pairs.value.flatMap{
      case(num,vec)=>
        val sp=vec.toSparse
        val bsv1 = new SparseVector[Double](sp.indices, sp.values, sp.size)
        List(num,bsv1)
    }

     // .foreach(println(_))
     // .foreach(i=>println(i._1))
     // .foreach(i=>println(i._2))
     // .map(_._2.toSparse)
    //  .foreach(println(_))

/*    num_idf_pairs.flatMap{
      case (i,j)=>
       List((i,j))
    }
      .foreach(println(_))*/

        val docSims = num_idf_pairs.flatMap {
          case (id1, idf1) =>
            //value获取广播变量的值,Array[(Long,Vector)]类型
            val idfs = b_num_idf_pairs.value
              //.filter(_._1!= id1)
            val sv1 = idf1.asInstanceOf[SV]
            val bsv1 = new SparseVector[Double](sv1.indices, sv1.values, sv1.size)
            idfs.map {
              case (id2, idf2) =>
                val sv2 = idf2.asInstanceOf[SV]
                val bsv2 = new SparseVector[Double](sv2.indices, sv2.values, sv2.size)
                val cosSim = bsv1.dot(bsv2)/ (norm(bsv1) * norm(bsv2))
                (id1, id2, cosSim)
           }
        }


    //docSims.foreach(i=>i.foreach(println(_)))


   // num_idf_pairs.foreach(println(_))
    docSims.foreach(println(_))


  }
}
